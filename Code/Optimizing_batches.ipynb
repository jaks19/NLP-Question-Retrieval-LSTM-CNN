{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "from get_q_matrices_functions import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.distance import CosineSimilarity\n",
    "\n",
    "\n",
    "''' Data Prep '''\n",
    "training_data = training_id_to_similar_different()\n",
    "trainingQuestionIds = list(training_data.keys())\n",
    "word2vec = get_words_and_embeddings()\n",
    "id2Data = questionID_to_questionData()\n",
    "\n",
    "\n",
    "''' Model Specs '''\n",
    "input_size = len(word2vec[list(word2vec.keys())[0]])\n",
    "hidden_size = 100\n",
    "num_layers = 1\n",
    "bias = True\n",
    "batch_first = True\n",
    "dropout = 0.2\n",
    "bidirectional = False\n",
    "\n",
    "lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "loss_function = torch.nn.MultiMarginLoss(margin=0.2)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=10**-4, weight_decay=0.001)\n",
    "\n",
    "h0 = Variable(torch.zeros(1, 1, hidden_size), requires_grad=True)\n",
    "c0 = Variable(torch.zeros(1, 1, hidden_size), requires_grad=True)\n",
    "\n",
    "\n",
    "''' Procedural parameters '''\n",
    "batch_size = 100\n",
    "num_differing_questions = 20\n",
    "\n",
    "num_epochs = 10\n",
    "num_batches = round(len(training_data.keys())/batch_size)\n",
    "\n",
    "\n",
    "'''Matrix constructors (use global vars, leave in order)'''\n",
    "\n",
    "# Given a list of ids, compute the hidden layer for each of those questions\n",
    "# Ideal if need to work on a group, not just one question\n",
    "def mean_pooled_hidden_layers_for_ids(list_ids, input_size):\n",
    "    qs_matrix_list = []\n",
    "    qs_seq_length = []\n",
    "        \n",
    "    for q in list_ids:\n",
    "        q_matrix_3d = get_question_matrix(q, word2vec, id2Data)\n",
    "        qs_matrix_list.append(q_matrix_3d)\n",
    "        qs_seq_length.append(q_matrix_3d.shape[1])\n",
    "    \n",
    "    qs_padded = padded_q_matrix(qs_seq_length, qs_matrix_list, input_size)\n",
    "    qs_hidden = torch.nn.utils.rnn.pad_packed_sequence(lstm(qs_padded, (h0, c0))[0], batch_first=True)\n",
    "    sum_h_qs = torch.sum(qs_hidden[0], dim=1)\n",
    "    \n",
    "    lst_hidden = []\n",
    "    for i in range(len(sum_h_qs)): \n",
    "        h = sum_h_qs[i] / qs_seq_length[i]\n",
    "        lst_hidden.append(h)\n",
    "    return lst_hidden\n",
    "\n",
    "# Given ids of main qs in this batch\n",
    "#\n",
    "# Returns:\n",
    "# 1. ids in ordered list as: \n",
    "# [\n",
    "# q_1+, q_1-, q_1--,..., q_1++, q_1-, q_1--,...,\n",
    "# q_2+, q_2-, q_2--,..., q_2++, q_2-, q_2--,...,\n",
    "# ...\n",
    "# ]\n",
    "# All n main questions have their pos,neg,neg,neg,... interleaved\n",
    "#\n",
    "# 2. A dict mapping main question id --> its interleaved sequence length\n",
    "\n",
    "def order_ids(q_ids):\n",
    "    global training_data\n",
    "    global num_differing_questions\n",
    "    \n",
    "    sequence_ids = []\n",
    "    dict_sequence_lengths = {}\n",
    "    \n",
    "    for q_main in q_ids:\n",
    "        p_pluses = training_data[q_main][0]\n",
    "        p_minuses = list(np.random.choice(training_data[q_main][1], num_differing_questions, replace = False))\n",
    "        sequence_length = len(p_pluses) * num_differing_questions + len(p_pluses)\n",
    "        dict_sequence_lengths[q_main] = sequence_length\n",
    "        for p_plus in p_pluses:\n",
    "            sequence_ids += [p_plus] + p_minuses\n",
    "\n",
    "    return sequence_ids, dict_sequence_lengths\n",
    "\n",
    "# sequence_ids, dict_sequence_lengths = order_ids([193,295137])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A tuple is (q+, q-, q--, q--- ...)\n",
    "# Let all main questions be set Q\n",
    "# Each q in Q has a number of tuples equal to number of positives |q+, q++, ...|\n",
    "# Each q in Q will have a 2D matrix of: num_tuples x num_candidates_in_tuple\n",
    "# Concatenate this matrix for all q in Q and you get a matrix of: |Q| x num_tuples x num_candidates_in_tuple\n",
    "\n",
    "# The above is for candidates\n",
    "# To do cosine_similarity, need same structure with q's\n",
    "# Basically each q will be a matrix of repeated q's: num_tuples x num_candidates_in_tuple, all elts are q (repeated)\n",
    "\n",
    "# This method constructs those matrices, use candidates=True for candidates matrix\n",
    "\n",
    "def construct_qs_matrix(q_ids_sequential, dict_sequence_lengths, candidates=False):\n",
    "    global lstm, h0, c0, word2vec, id2data, input_size, num_differing_questions\n",
    "    \n",
    "    if not candidates:\n",
    "        q_ids_complete = []\n",
    "        for q in q_ids_sequential:\n",
    "            q_ids_complete += [q] * dict_sequence_lengths[q]\n",
    "    \n",
    "    else: q_ids_complete = q_ids_sequential\n",
    "\n",
    "    qs_matrix_list = []\n",
    "    qs_seq_length = []\n",
    "    \n",
    "    for q in q_ids_complete:\n",
    "        q_matrix_3d = get_question_matrix(q, word2vec, id2Data)\n",
    "        qs_matrix_list.append(q_matrix_3d)\n",
    "        qs_seq_length.append(q_matrix_3d.shape[1])\n",
    "\n",
    "    qs_padded = padded_q_matrix(qs_seq_length, qs_matrix_list, input_size)\n",
    "    qs_hidden = torch.nn.utils.rnn.pad_packed_sequence(lstm(qs_padded, (h0, c0))[0], batch_first=True)\n",
    "    sum_h_qs = torch.sum(qs_hidden[0], dim=1)\n",
    "    mean_pooled_h_qs = torch.div(sum_h_qs, torch.autograd.Variable(torch.FloatTensor(qs_seq_length)[:, np.newaxis]))\n",
    "    \n",
    "    qs_tuples = mean_pooled_h_qs.split(1+num_differing_questions)\n",
    "    final_matrix_tuples_by_constituent_qs_by_hidden_size = torch.stack(qs_tuples, dim=0, out=None)\n",
    "\n",
    "    return final_matrix_tuples_by_constituent_qs_by_hidden_size\n",
    "\n",
    "# q_ids_this_batch = [193,295137]\n",
    "# print(construct_qs_matrix(q_ids_this_batch, dict_sequence_lengths, candidates=False))\n",
    "# candidate_sequential_ids_this_batch = sequence_ids\n",
    "# print(construct_qs_matrix(candidate_sequential_ids_this_batch, dict_sequence_lengths, candidates=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on batch #:  1\n"
     ]
    }
   ],
   "source": [
    "'''Begin training'''\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(1, num_batches+2):\n",
    "        \n",
    "        print(\"Working on batch #: \", batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        questions_this_batch = trainingQuestionIds[batch_size * (batch - 1):batch_size * batch]\n",
    "        sequence_ids, dict_sequence_lengths = order_ids(questions_this_batch)\n",
    "        \n",
    "        main_qs_tuples_matrix = construct_qs_matrix(questions_this_batch, dict_sequence_lengths, candidates=False)\n",
    "        candidates_qs_tuples_matrix = construct_qs_matrix(sequence_ids, dict_sequence_lengths, candidates=True)\n",
    "        \n",
    "        similarity_matrix = torch.nn.functional.cosine_similarity(candidates_qs_tuples_matrix, main_qs_tuples_matrix, dim=2, eps=1e-08)\n",
    "        target = Variable(torch.LongTensor([0] * int(len(sequence_ids)/(1+num_differing_questions))))\n",
    "        loss_batch = loss_function(similarity_matrix, target)\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"loss on this batch: \", loss_batch.data[0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:3]",
   "language": "python",
   "name": "conda-env-3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
