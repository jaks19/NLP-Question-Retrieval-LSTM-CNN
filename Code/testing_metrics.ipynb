{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on batch #:  1\n",
      "loss_on_batch: 0.14596445858478546  time_on_batch: 0.6497280597686768\n",
      "Working on batch #:  2\n",
      "loss_on_batch: 0.14669625461101532  time_on_batch: 0.7349562644958496\n",
      "Working on batch #:  3\n",
      "loss_on_batch: 0.1289166659116745  time_on_batch: 0.6637654304504395\n",
      "Working on batch #:  4\n",
      "loss_on_batch: 0.11793899536132812  time_on_batch: 0.6487267017364502\n",
      "Working on batch #:  5\n",
      "loss_on_batch: 0.0795331820845604  time_on_batch: 0.6647689342498779\n",
      "Working on batch #:  6\n",
      "loss_on_batch: 0.05551956966519356  time_on_batch: 1.4538679122924805\n",
      "Working on batch #:  7\n",
      "loss_on_batch: 0.02538386732339859  time_on_batch: 0.7269339561462402\n",
      "Working on batch #:  8\n",
      "loss_on_batch: 0.20426757633686066  time_on_batch: 5.276035785675049\n",
      "Working on batch #:  9\n",
      "loss_on_batch: 0.005092999432235956  time_on_batch: 1.1189782619476318\n",
      "Working on batch #:  10\n",
      "loss_on_batch: 0.2975258231163025  time_on_batch: 0.844245195388794\n",
      "Working on batch #:  11\n",
      "loss_on_batch: 0.11064089834690094  time_on_batch: 0.6747944355010986\n",
      "Working on batch #:  12\n",
      "loss_on_batch: 0.07491971552371979  time_on_batch: 0.6547410488128662\n",
      "Working on batch #:  13\n",
      "loss_on_batch: 0.03884313628077507  time_on_batch: 0.6447155475616455\n",
      "Working on batch #:  14\n",
      "loss_on_batch: 0.1914955973625183  time_on_batch: 1.399724006652832\n",
      "Working on batch #:  15\n",
      "loss_on_batch: 0.08487924933433533  time_on_batch: 0.6477231979370117\n",
      "Working on batch #:  16\n",
      "loss_on_batch: 0.1983332633972168  time_on_batch: 0.7369613647460938\n",
      "Working on batch #:  17\n",
      "loss_on_batch: 0.15148594975471497  time_on_batch: 0.6818125247955322\n",
      "Working on batch #:  18\n",
      "loss_on_batch: 0.14219196140766144  time_on_batch: 1.0878946781158447\n",
      "Working on batch #:  19\n",
      "loss_on_batch: 0.09397871792316437  time_on_batch: 0.7239267826080322\n",
      "Working on batch #:  20\n",
      "loss_on_batch: 0.14430442452430725  time_on_batch: 1.0708489418029785\n",
      "Working on batch #:  21\n",
      "loss_on_batch: 0.11731080710887909  time_on_batch: 0.7229244709014893\n",
      "Working on batch #:  22\n",
      "loss_on_batch: 0.09233515709638596  time_on_batch: 0.646721363067627\n",
      "Working on batch #:  23\n",
      "loss_on_batch: 0.07517088949680328  time_on_batch: 1.3415696620941162\n",
      "Working on batch #:  24\n",
      "loss_on_batch: 0.21131740510463715  time_on_batch: 0.6607580184936523\n",
      "Working on batch #:  25\n",
      "loss_on_batch: 0.15444828569889069  time_on_batch: 1.049793004989624\n",
      "Working on batch #:  26\n",
      "loss_on_batch: 0.12018025666475296  time_on_batch: 3.797102212905884\n",
      "Working on batch #:  27\n",
      "loss_on_batch: 0.14923100173473358  time_on_batch: 5.625967264175415\n",
      "Working on batch #:  28\n",
      "loss_on_batch: 0.08223513513803482  time_on_batch: 0.7008650302886963\n",
      "Working on batch #:  29\n",
      "loss_on_batch: 0.0884922593832016  time_on_batch: 0.7229235172271729\n",
      "Working on batch #:  30\n",
      "loss_on_batch: 0.15751639008522034  time_on_batch: 4.608259916305542\n",
      "Working on batch #:  31\n",
      "loss_on_batch: 0.13523566722869873  time_on_batch: 0.7399678230285645\n",
      "Working on batch #:  32\n",
      "loss_on_batch: 0.10109344869852066  time_on_batch: 0.6537396907806396\n",
      "Working on batch #:  33\n",
      "loss_on_batch: 0.1039411649107933  time_on_batch: 0.9615581035614014\n",
      "Working on batch #:  34\n",
      "loss_on_batch: 0.027892032638192177  time_on_batch: 0.6998627185821533\n",
      "Working on batch #:  35\n",
      "loss_on_batch: 0.10452448576688766  time_on_batch: 0.715904951095581\n",
      "Working on batch #:  36\n",
      "loss_on_batch: 0.06783871352672577  time_on_batch: 1.0197131633758545\n",
      "Working on batch #:  37\n",
      "loss_on_batch: 0.13870270550251007  time_on_batch: 0.7048735618591309\n",
      "Working on batch #:  38\n",
      "loss_on_batch: 0.04180552065372467  time_on_batch: 0.7309441566467285\n",
      "Working on batch #:  39\n",
      "loss_on_batch: 0.1188703253865242  time_on_batch: 0.9735903739929199\n",
      "Working on batch #:  40\n",
      "loss_on_batch: 0.0981244221329689  time_on_batch: 1.3927066326141357\n",
      "Working on batch #:  41\n",
      "loss_on_batch: 0.029831858351826668  time_on_batch: 1.0207161903381348\n",
      "Working on batch #:  42\n",
      "loss_on_batch: 0.0790746808052063  time_on_batch: 0.7329506874084473\n",
      "Working on batch #:  43\n",
      "loss_on_batch: 0.1921268254518509  time_on_batch: 0.6737926006317139\n",
      "Working on batch #:  44\n",
      "loss_on_batch: 0.057981643825769424  time_on_batch: 1.25032639503479\n",
      "Working on batch #:  45\n",
      "loss_on_batch: 0.04275413602590561  time_on_batch: 0.6066138744354248\n",
      "Working on batch #:  46\n",
      "loss_on_batch: 0.05800161138176918  time_on_batch: 1.0217175483703613\n",
      "Working on batch #:  47\n",
      "loss_on_batch: 0.1507379561662674  time_on_batch: 1.0868911743164062\n",
      "Working on batch #:  48\n",
      "loss_on_batch: 0.02698681131005287  time_on_batch: 0.6808109283447266\n",
      "Working on batch #:  49\n",
      "loss_on_batch: 0.06119367480278015  time_on_batch: 2.533740758895874\n",
      "Working on batch #:  50\n",
      "loss_on_batch: 0.1265682578086853  time_on_batch: 1.0387659072875977\n",
      "MRR score on dev set: 0.6094124474559256\n",
      "MRR score on test set: 0.5981942310096959\n",
      "MAP score on dev set: 0.493123905293\n",
      "MAP score on test set: 0.469420028689\n",
      "Precision at 1 score on dev set: 0.4\n",
      "Precision at 1 score on test set: 0.4\n",
      "Precision at 5 score on dev set: 0.33199999999999996\n",
      "Precision at 5 score on test set: 0.33300000000000024\n",
      "Working on batch #:  1\n",
      "loss_on_batch: 0.026367278769612312  time_on_batch: 0.8101556301116943\n",
      "Working on batch #:  2\n",
      "loss_on_batch: 0.02562400884926319  time_on_batch: 0.6768016815185547\n",
      "Working on batch #:  3\n",
      "loss_on_batch: 0.020680036395788193  time_on_batch: 0.6788041591644287\n",
      "Working on batch #:  4\n",
      "loss_on_batch: 0.013706058263778687  time_on_batch: 0.6366937160491943\n",
      "Working on batch #:  5\n",
      "loss_on_batch: 0.001997805666178465  time_on_batch: 0.6477227210998535\n",
      "Working on batch #:  6\n",
      "loss_on_batch: 0.010905618779361248  time_on_batch: 1.3987207412719727\n",
      "Working on batch #:  7\n",
      "loss_on_batch: 0.006522155832499266  time_on_batch: 0.7008647918701172\n",
      "Working on batch #:  8\n",
      "loss_on_batch: 0.02215793915092945  time_on_batch: 5.020355939865112\n",
      "Working on batch #:  9\n",
      "loss_on_batch: 0.001001200987957418  time_on_batch: 1.0377614498138428\n",
      "Working on batch #:  10\n",
      "loss_on_batch: 0.020032402127981186  time_on_batch: 0.681814432144165\n",
      "Working on batch #:  11\n",
      "loss_on_batch: 0.014822938479483128  time_on_batch: 0.6637661457061768\n",
      "Working on batch #:  12\n",
      "loss_on_batch: 0.007749500684440136  time_on_batch: 0.6266674995422363\n",
      "Working on batch #:  13\n",
      "loss_on_batch: 0.003902910742908716  time_on_batch: 0.6798083782196045\n",
      "Working on batch #:  14\n",
      "loss_on_batch: 0.06642166525125504  time_on_batch: 1.3766632080078125\n",
      "Working on batch #:  15\n",
      "loss_on_batch: 0.002040099585428834  time_on_batch: 0.6397030353546143\n",
      "Working on batch #:  16\n",
      "loss_on_batch: 0.02014854922890663  time_on_batch: 0.7459850311279297\n",
      "Working on batch #:  17\n",
      "loss_on_batch: 0.011864935979247093  time_on_batch: 0.6427111625671387\n",
      "Working on batch #:  18\n",
      "loss_on_batch: 0.01725998893380165  time_on_batch: 1.0437777042388916\n",
      "Working on batch #:  19\n",
      "loss_on_batch: 0.012296844273805618  time_on_batch: 0.7620284557342529\n",
      "Working on batch #:  20\n",
      "loss_on_batch: 0.049060940742492676  time_on_batch: 1.3876910209655762\n",
      "Working on batch #:  21\n",
      "loss_on_batch: 0.05774690955877304  time_on_batch: 0.6998624801635742\n",
      "Working on batch #:  22\n",
      "loss_on_batch: 0.03889532759785652  time_on_batch: 0.6727917194366455\n",
      "Working on batch #:  23\n",
      "loss_on_batch: 0.016187680885195732  time_on_batch: 1.3305399417877197\n",
      "Working on batch #:  24\n",
      "loss_on_batch: 0.12483451515436172  time_on_batch: 0.7369606494903564\n",
      "Working on batch #:  25\n",
      "loss_on_batch: 0.04380665719509125  time_on_batch: 0.9876317977905273\n",
      "Working on batch #:  26\n",
      "loss_on_batch: 0.044103361666202545  time_on_batch: 4.024706602096558\n",
      "Working on batch #:  27\n",
      "loss_on_batch: 0.09641562402248383  time_on_batch: 6.068143367767334\n",
      "Working on batch #:  28\n",
      "loss_on_batch: 0.012627022340893745  time_on_batch: 0.6868283748626709\n",
      "Working on batch #:  29\n",
      "loss_on_batch: 0.01586436852812767  time_on_batch: 0.6948473453521729\n",
      "Working on batch #:  30\n",
      "loss_on_batch: 0.03572118282318115  time_on_batch: 4.614275932312012\n",
      "Working on batch #:  31\n",
      "loss_on_batch: 0.03374439477920532  time_on_batch: 0.6226565837860107\n",
      "Working on batch #:  32\n",
      "loss_on_batch: 0.00556676322594285  time_on_batch: 0.6507313251495361\n",
      "Working on batch #:  33\n",
      "loss_on_batch: 0.030791856348514557  time_on_batch: 1.0066792964935303\n",
      "Working on batch #:  34\n",
      "loss_on_batch: 0.0013820216991007328  time_on_batch: 0.6397018432617188\n",
      "Working on batch #:  35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_on_batch: 0.015346010215580463  time_on_batch: 0.7028698921203613\n",
      "Working on batch #:  36\n",
      "loss_on_batch: 0.005374924745410681  time_on_batch: 1.0307416915893555\n",
      "Working on batch #:  37\n",
      "loss_on_batch: 0.006092637777328491  time_on_batch: 0.684821605682373\n",
      "Working on batch #:  38\n",
      "loss_on_batch: 0.04227466881275177  time_on_batch: 0.6888329982757568\n",
      "Working on batch #:  39\n",
      "loss_on_batch: 0.08055295050144196  time_on_batch: 1.0006625652313232\n",
      "Working on batch #:  40\n",
      "loss_on_batch: 0.027445100247859955  time_on_batch: 1.666433334350586\n",
      "Working on batch #:  41\n",
      "loss_on_batch: 0.009228919632732868  time_on_batch: 1.0999250411987305\n",
      "Working on batch #:  42\n",
      "loss_on_batch: 0.005674054380506277  time_on_batch: 0.6737926006317139\n",
      "Working on batch #:  43\n",
      "loss_on_batch: 0.04674356058239937  time_on_batch: 0.7560112476348877\n",
      "Working on batch #:  44\n",
      "loss_on_batch: 0.02065519243478775  time_on_batch: 1.2844173908233643\n",
      "Working on batch #:  45\n",
      "loss_on_batch: 0.00786089152097702  time_on_batch: 0.6647689342498779\n",
      "Working on batch #:  46\n",
      "loss_on_batch: 0.04958914592862129  time_on_batch: 1.1410367488861084\n",
      "Working on batch #:  47\n",
      "loss_on_batch: 0.03600611910223961  time_on_batch: 1.230273723602295\n",
      "Working on batch #:  48\n",
      "loss_on_batch: 0.021500086411833763  time_on_batch: 1.0678415298461914\n",
      "Working on batch #:  49\n",
      "loss_on_batch: 0.041877686977386475  time_on_batch: 3.446162223815918\n",
      "Working on batch #:  50\n",
      "loss_on_batch: 0.059241846203804016  time_on_batch: 1.349590539932251\n",
      "MRR score on dev set: 0.6795054583098061\n",
      "MRR score on test set: 0.5884532427376727\n",
      "MAP score on dev set: 0.516579103826\n",
      "MAP score on test set: 0.473057810788\n",
      "Precision at 1 score on dev set: 0.5\n",
      "Precision at 1 score on test set: 0.39\n",
      "Precision at 5 score on dev set: 0.344\n",
      "Precision at 5 score on test set: 0.3190000000000002\n",
      "Working on batch #:  1\n",
      "loss_on_batch: 0.002862277440726757  time_on_batch: 0.6858253479003906\n",
      "Working on batch #:  2\n",
      "loss_on_batch: 0.0038691232912242413  time_on_batch: 0.6557433605194092\n",
      "Working on batch #:  3\n",
      "loss_on_batch: 0.002590679097920656  time_on_batch: 0.5965874195098877\n",
      "Working on batch #:  4\n",
      "loss_on_batch: 0.005267818924039602  time_on_batch: 0.6597552299499512\n",
      "Working on batch #:  5\n",
      "loss_on_batch: 0.006747975014150143  time_on_batch: 0.6386985778808594\n",
      "Working on batch #:  6\n",
      "loss_on_batch: 0.0016101201763376594  time_on_batch: 1.3225183486938477\n",
      "Working on batch #:  7\n",
      "loss_on_batch: 0.0029486701823771  time_on_batch: 0.6427109241485596\n",
      "Working on batch #:  8\n",
      "loss_on_batch: 0.01304734405130148  time_on_batch: 4.763675928115845\n",
      "Working on batch #:  9\n",
      "loss_on_batch: 0.0  time_on_batch: 0.9946465492248535\n",
      "Working on batch #:  10\n",
      "loss_on_batch: 0.022740235552191734  time_on_batch: 0.6507320404052734\n",
      "Working on batch #:  11\n",
      "loss_on_batch: 0.007062952965497971  time_on_batch: 0.6226568222045898\n",
      "Working on batch #:  12\n",
      "loss_on_batch: 0.007534194737672806  time_on_batch: 0.6687791347503662\n",
      "Working on batch #:  13\n",
      "loss_on_batch: 0.0  time_on_batch: 0.7118940353393555\n",
      "Working on batch #:  14\n",
      "loss_on_batch: 0.0018949416698887944  time_on_batch: 1.2603533267974854\n",
      "Working on batch #:  15\n",
      "loss_on_batch: 0.0036331203300505877  time_on_batch: 0.6276702880859375\n",
      "Working on batch #:  16\n",
      "loss_on_batch: 0.021167686209082603  time_on_batch: 0.6768012046813965\n",
      "Working on batch #:  17\n",
      "loss_on_batch: 0.013671687804162502  time_on_batch: 0.6407046318054199\n",
      "Working on batch #:  18\n",
      "loss_on_batch: 0.015682149678468704  time_on_batch: 1.000661849975586\n",
      "Working on batch #:  19\n",
      "loss_on_batch: 0.014044882729649544  time_on_batch: 0.6386988162994385\n",
      "Working on batch #:  20\n",
      "loss_on_batch: 0.020054573193192482  time_on_batch: 0.9836184978485107\n",
      "Working on batch #:  21\n",
      "loss_on_batch: 0.009615873917937279  time_on_batch: 0.6306781768798828\n",
      "Working on batch #:  22\n",
      "loss_on_batch: 0.007005035877227783  time_on_batch: 0.6206507682800293\n",
      "Working on batch #:  23\n",
      "loss_on_batch: 0.0034784290473908186  time_on_batch: 1.2413020133972168\n",
      "Working on batch #:  24\n",
      "loss_on_batch: 0.005715135484933853  time_on_batch: 0.6196486949920654\n",
      "Working on batch #:  25\n",
      "loss_on_batch: 0.007893114350736141  time_on_batch: 1.0237250328063965\n",
      "Working on batch #:  26\n",
      "loss_on_batch: 0.024680012837052345  time_on_batch: 3.557464599609375\n",
      "Working on batch #:  27\n",
      "loss_on_batch: 0.10726607590913773  time_on_batch: 5.407386064529419\n",
      "Working on batch #:  28\n",
      "loss_on_batch: 0.015541407279670238  time_on_batch: 0.6567487716674805\n",
      "Working on batch #:  29\n",
      "loss_on_batch: 0.005214834585785866  time_on_batch: 0.6356923580169678\n",
      "Working on batch #:  30\n",
      "loss_on_batch: 0.0495699942111969  time_on_batch: 4.281391143798828\n",
      "Working on batch #:  31\n",
      "loss_on_batch: 0.025368979200720787  time_on_batch: 0.6497280597686768\n",
      "Working on batch #:  32\n",
      "loss_on_batch: 0.013874777592718601  time_on_batch: 0.6336865425109863\n",
      "Working on batch #:  33\n",
      "loss_on_batch: 0.0033583082258701324  time_on_batch: 1.0558087825775146\n",
      "Working on batch #:  34\n",
      "loss_on_batch: 0.00625571608543396  time_on_batch: 0.7439806461334229\n",
      "Working on batch #:  35\n",
      "loss_on_batch: 0.0072305514477193356  time_on_batch: 0.6978561878204346\n",
      "Working on batch #:  36\n",
      "loss_on_batch: 0.0007366358186118305  time_on_batch: 1.0377612113952637\n",
      "Working on batch #:  37\n",
      "loss_on_batch: 0.0018165920628234744  time_on_batch: 0.6527366638183594\n",
      "Working on batch #:  38\n",
      "loss_on_batch: 0.011163475923240185  time_on_batch: 0.6366946697235107\n",
      "Working on batch #:  39\n",
      "loss_on_batch: 0.016530843451619148  time_on_batch: 1.0247282981872559\n",
      "Working on batch #:  40\n",
      "loss_on_batch: 0.02880440652370453  time_on_batch: 1.6072752475738525\n",
      "Working on batch #:  41\n",
      "loss_on_batch: 0.0  time_on_batch: 1.1189777851104736\n",
      "Working on batch #:  42\n",
      "loss_on_batch: 0.019291987642645836  time_on_batch: 0.7148964405059814\n",
      "Working on batch #:  43\n",
      "loss_on_batch: 0.02468201145529747  time_on_batch: 0.7991287708282471\n",
      "Working on batch #:  44\n",
      "loss_on_batch: 0.023869752883911133  time_on_batch: 1.5100176334381104\n",
      "Working on batch #:  45\n",
      "loss_on_batch: 0.004538299050182104  time_on_batch: 0.7088878154754639\n",
      "Working on batch #:  46\n",
      "loss_on_batch: 0.010045911185443401  time_on_batch: 1.0758609771728516\n",
      "Working on batch #:  47\n",
      "loss_on_batch: 0.0034958766773343086  time_on_batch: 1.0177078247070312\n",
      "Working on batch #:  48\n",
      "loss_on_batch: 0.005466055124998093  time_on_batch: 0.6998617649078369\n",
      "Working on batch #:  49\n",
      "loss_on_batch: 0.012468780390918255  time_on_batch: 2.5959064960479736\n",
      "Working on batch #:  50\n",
      "loss_on_batch: 0.013861186802387238  time_on_batch: 1.0357542037963867\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-54e2f919a860>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;31m# Evaluate on dev and test sets for MRR score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mdev_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_question_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2Data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_question_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2Data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MRR score on dev set:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MRR score on test set:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-54e2f919a860>\u001b[0m in \u001b[0;36meval_model\u001b[1;34m(cnn, ids, data, word2vec, id2Data, truncation_val)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0msequence_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_pluses_indices_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morganize_test_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[0mcandidates_qs_tuples_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstruct_qs_matrix_testing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2Data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_differing_questions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m     \u001b[0mmain_qs_tuples_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstruct_qs_matrix_testing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2Data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_differing_questions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\Harvard Grad school\\2017-2018 Academic Year\\Fall 2017\\Advanced Natural Language Processing (MIT EECS 6.864)\\6.864 Project\\6.864 Project Part 1\\Code\\cnn_utils.py\u001b[0m in \u001b[0;36mconstruct_qs_matrix_testing\u001b[1;34m(q_ids_sequential, cnn, word2vec, id2Data, input_size, num_differing_questions, truncation_val, candidates)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[0mqs_padded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_matrix_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[0mqs_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[0msum_h_qs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m     \u001b[0mmean_pooled_h_qs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_h_qs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_seq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0mqs_tuples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_pooled_h_qs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_differing_questions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self, dim, keepdim)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\autograd\\_functions\\reduce.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, input, dim, keepdim)\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from preprocess import *\n",
    "from scoring_metrics import *\n",
    "from cnn_utils import *\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "\n",
    "saved_model_name = \"bestbest_cnn\"\n",
    "\n",
    "'''Hyperparams dashboard'''\n",
    "margin = 0.2\n",
    "lr = 10**-3\n",
    "truncation_val = 50\n",
    "\n",
    "\n",
    "''' Data Prep '''\n",
    "word2vec = get_words_and_embeddings()\n",
    "id2Data = questionID_to_questionData_truncate(truncation_val)\n",
    "\n",
    "training_data = training_id_to_similar_different()\n",
    "trainingQuestionIds = list(training_data.keys())[:100]\n",
    "\n",
    "dev_data = devTest_id_to_similar_different(dev=True)\n",
    "dev_question_ids = list(dev_data.keys())[:50]\n",
    "\n",
    "test_data = devTest_id_to_similar_different(dev=False)\n",
    "test_question_ids = list(test_data.keys())\n",
    "\n",
    "\n",
    "''' Model Specs '''\n",
    "# CNN parameters\n",
    "input_size = len(word2vec[list(word2vec.keys())[0]])\n",
    "hidden_size = 667\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 0\n",
    "dilation = 1\n",
    "groups = 1\n",
    "bias = True\n",
    "\n",
    "# CNN model\n",
    "cnn = torch.nn.Sequential()\n",
    "cnn.add_module('conv', torch.nn.Conv1d(in_channels = 200, out_channels = hidden_size, kernel_size = kernel_size, padding = padding, dilation = dilation, groups = groups, bias = bias))\n",
    "cnn.add_module('tanh', torch.nn.Tanh())\n",
    "\n",
    "# Loss function\n",
    "loss_function = torch.nn.MultiMarginLoss(margin=margin)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=lr, weight_decay=0.001)\n",
    "\n",
    "\n",
    "''' Procedural parameters '''\n",
    "batch_size = 2\n",
    "num_differing_questions = 20\n",
    "num_epochs = 10\n",
    "num_batches = round(len(trainingQuestionIds)/batch_size)\n",
    "\n",
    "\n",
    "def train_model(cnn, optimizer, batch_ids, batch_data, word2vec, id2Data, truncation_val):\n",
    "    cnn.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    sequence_ids, dict_sequence_lengths = organize_ids_training(batch_ids, batch_data, num_differing_questions)\n",
    "\n",
    "    candidates_qs_tuples_matrix = construct_qs_matrix_training(sequence_ids, cnn, word2vec, id2Data, dict_sequence_lengths, input_size, num_differing_questions, truncation_val, candidates=True)\n",
    "    main_qs_tuples_matrix = construct_qs_matrix_training(batch_ids, cnn, word2vec, id2Data, dict_sequence_lengths, input_size, num_differing_questions, truncation_val, candidates=False)\n",
    "    similarity_matrix = torch.nn.functional.cosine_similarity(candidates_qs_tuples_matrix, main_qs_tuples_matrix, dim=2, eps=1e-08)\n",
    "\n",
    "    target = Variable(torch.LongTensor([0] * int(len(sequence_ids)/(1+num_differing_questions))))\n",
    "    loss_batch = loss_function(similarity_matrix, target)\n",
    "\n",
    "    loss_batch.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"loss_on_batch:\", loss_batch.data[0], \" time_on_batch:\", time.time() - start)\n",
    "    return\n",
    "\n",
    "\n",
    "def eval_model(cnn, ids, data, word2vec, id2Data, truncation_val):\n",
    "    cnn.eval()\n",
    "    sequence_ids, p_pluses_indices_dict = organize_test_ids(ids, data)\n",
    "\n",
    "    candidates_qs_tuples_matrix = construct_qs_matrix_testing(sequence_ids, cnn, word2vec, id2Data, input_size, num_differing_questions, truncation_val, candidates=True)\n",
    "    main_qs_tuples_matrix = construct_qs_matrix_testing(ids, cnn, word2vec, id2Data, input_size, num_differing_questions, truncation_val, candidates=False)\n",
    "\n",
    "    similarity_matrix = torch.nn.functional.cosine_similarity(candidates_qs_tuples_matrix, main_qs_tuples_matrix, dim=2, eps=1e-08)\n",
    "    MRR_score = get_MRR_score(similarity_matrix, p_pluses_indices_dict)\n",
    "    MAP_score = get_MAP_score(similarity_matrix, p_pluses_indices_dict)\n",
    "    avg_prec_at_1 = avg_precision_at_k(similarity_matrix, p_pluses_indices_dict, 1)\n",
    "    avg_prec_at_5 = avg_precision_at_k(similarity_matrix, p_pluses_indices_dict, 5)\n",
    "    return MRR_score, MAP_score, avg_prec_at_1, avg_prec_at_5\n",
    "\n",
    "\n",
    "'''Begin training'''\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Train on whole training data set\n",
    "    for batch in range(1, num_batches+1):\n",
    "        start = time.time()\n",
    "        questions_this_training_batch = trainingQuestionIds[batch_size * (batch - 1):batch_size * batch]\n",
    "        print(\"Working on batch #: \", batch)\n",
    "        train_model(cnn, optimizer, questions_this_training_batch, training_data, word2vec, id2Data, truncation_val)\n",
    "        \n",
    "    # Evaluate on dev and test sets for MRR score\n",
    "    dev_scores = eval_model(cnn, dev_question_ids, dev_data, word2vec, id2Data, truncation_val)\n",
    "    test_scores = eval_model(cnn, test_question_ids, test_data, word2vec, id2Data, truncation_val)\n",
    "    print(\"MRR score on dev set:\", dev_scores[0])\n",
    "    print(\"MRR score on test set:\", test_scores[0])\n",
    "    print(\"MAP score on dev set:\", dev_scores[1])\n",
    "    print(\"MAP score on test set:\", test_scores[1])\n",
    "    print(\"Precision at 1 score on dev set:\", dev_scores[2])\n",
    "    print(\"Precision at 1 score on test set:\", test_scores[2])\n",
    "    print(\"Precision at 5 score on dev set:\", dev_scores[3])\n",
    "    print(\"Precision at 5 score on test set:\", test_scores[3])\n",
    "\n",
    "    # Log results to local logs.txt file\n",
    "    #with open('logs_cnn.txt', 'a') as log_file:\n",
    "    #    log_file.write('epoch: ' + str(epoch) + '\\n')\n",
    "    #    log_file.write('lr: ' + str(lr) +  ' marg: ' + str(margin) + '\\n' )        \n",
    "    #    log_file.write('dev_MRR: ' +  str(dev_scores[0]) + '\\n')\n",
    "    #    log_file.write('test_MRR: ' +  str(test_scores[0]) + '\\n')\n",
    "    #    log_file.write('dev_MAP: ' +  str(dev_scores[1]) + '\\n')\n",
    "    #    log_file.write('test_MAP: ' +  str(test_scores[1]) + '\\n')\n",
    "    #    log_file.write('dev_p_at_1: ' +  str(dev_scores[2]) + '\\n')\n",
    "    #    log_file.write('test_p_at_1: ' +  str(test_scores[2]) + '\\n')\n",
    "    #    log_file.write('dev_p_at_5: ' +  str(dev_scores[3]) + '\\n')\n",
    "    #    log_file.write('test_p_at_5: ' +  str(test_scores[3]) + '\\n')\n",
    "\n",
    "    # Save model for this epoch\n",
    "    #torch.save(cnn, '../Pickle/' + saved_model_name + '_epoch' + str(epoch) + '.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
