{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.distance import CosineSimilarity\n",
    "\n",
    "import time\n",
    "\n",
    "# Produces tensor [1 x num_words x input_size] for one particular question\n",
    "def get_question_matrix(questionID, word2vec, id2Data, input_size, truncation_val):\n",
    "    # Get the vector representation for each word in this question as list [v1,v2,v3,...]\n",
    "    q_word_vecs = []\n",
    "    for word in id2Data[questionID]:\n",
    "        try:\n",
    "            word_vec = np.array(word2vec[word]).astype(np.float32).reshape(len(word2vec[word]), -1)\n",
    "            q_word_vecs.append(word_vec)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    # num_words x dim_words\n",
    "    q_matrix = torch.Tensor(np.concatenate(q_word_vecs, axis=1).T)\n",
    "    num_words_found = q_matrix.size()[0]\n",
    "\n",
    "    if num_words_found < truncation_val:\n",
    "        padding_rows = torch.zeros(truncation_val-num_words_found, input_size)\n",
    "        q_matrix = torch.cat((q_matrix, padding_rows), 0)\n",
    "\n",
    "    return [q_matrix.unsqueeze(0), num_words_found]\n",
    "\n",
    "\n",
    "# Take list of question tensors and makes a batch of all of them [batch_size x num_questions_in_batch x input_size]\n",
    "def padded_q_matrix(q_matrix_list):\n",
    "    return Variable(torch.cat(q_matrix_list, 0))\n",
    "\n",
    "\n",
    "''' Data Prep '''\n",
    "training_data = training_id_to_similar_different()\n",
    "trainingQuestionIds = list(training_data.keys())\n",
    "word2vec = get_words_and_embeddings()\n",
    "id2Data = questionID_to_questionData_truncate(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on batch #:  1\n",
      "loss_on_batch: 0.1852787733078003  time_on_batch: 72.3687093257904\n",
      "Working on batch #:  2\n",
      "loss_on_batch: 0.11037001758813858  time_on_batch: 142.6866376399994\n",
      "Working on batch #:  3\n",
      "loss_on_batch: 0.17380912601947784  time_on_batch: 100.35934805870056\n",
      "Working on batch #:  4\n",
      "loss_on_batch: 0.17231303453445435  time_on_batch: 77.33512353897095\n",
      "Working on batch #:  5\n",
      "loss_on_batch: 0.17525404691696167  time_on_batch: 117.04152965545654\n",
      "Working on batch #:  6\n",
      "loss_on_batch: 0.17258015275001526  time_on_batch: 65.9485433101654\n",
      "Working on batch #:  7\n",
      "loss_on_batch: 0.1699548065662384  time_on_batch: 64.75343918800354\n",
      "Working on batch #:  8\n",
      "loss_on_batch: 0.16704756021499634  time_on_batch: 56.31921410560608\n",
      "Working on batch #:  9\n",
      "loss_on_batch: 0.1718629151582718  time_on_batch: 46.24176621437073\n",
      "Working on batch #:  10\n",
      "loss_on_batch: 0.17305846512317657  time_on_batch: 68.52842307090759\n",
      "Working on batch #:  11\n",
      "loss_on_batch: 0.17407797276973724  time_on_batch: 51.17994809150696\n",
      "Working on batch #:  12\n",
      "loss_on_batch: 0.17067059874534607  time_on_batch: 83.8030800819397\n",
      "Working on batch #:  13\n",
      "loss_on_batch: 0.16533862054347992  time_on_batch: 52.781145095825195\n",
      "Working on batch #:  14\n",
      "loss_on_batch: 0.1625533252954483  time_on_batch: 54.963533878326416\n",
      "Working on batch #:  15\n",
      "loss_on_batch: 0.16447925567626953  time_on_batch: 54.31178021430969\n",
      "Working on batch #:  16\n",
      "loss_on_batch: 0.15851859748363495  time_on_batch: 65.6976261138916\n",
      "Working on batch #:  17\n",
      "loss_on_batch: 0.142551451921463  time_on_batch: 80.66584801673889\n",
      "Working on batch #:  18\n",
      "loss_on_batch: 0.16376180946826935  time_on_batch: 162.17196941375732\n",
      "Working on batch #:  19\n",
      "loss_on_batch: 0.13233062624931335  time_on_batch: 73.18693161010742\n",
      "Working on batch #:  20\n",
      "loss_on_batch: 0.12464677542448044  time_on_batch: 75.60599160194397\n",
      "Working on batch #:  21\n",
      "loss_on_batch: 0.11815286427736282  time_on_batch: 82.4000837802887\n",
      "Working on batch #:  22\n",
      "loss_on_batch: 0.10925029218196869  time_on_batch: 71.72088289260864\n",
      "Working on batch #:  23\n",
      "loss_on_batch: 0.1264403611421585  time_on_batch: 56.64483857154846\n",
      "Working on batch #:  24\n",
      "loss_on_batch: 0.1274494081735611  time_on_batch: 73.02489876747131\n",
      "Working on batch #:  25\n",
      "loss_on_batch: 0.12915551662445068  time_on_batch: 126.08537936210632\n",
      "Working on batch #:  26\n",
      "loss_on_batch: 0.11327967047691345  time_on_batch: 189.23140573501587\n",
      "Working on batch #:  27\n",
      "loss_on_batch: 0.11104119569063187  time_on_batch: 75.56323575973511\n",
      "Working on batch #:  28\n",
      "loss_on_batch: 0.07932277768850327  time_on_batch: 196.08514523506165\n",
      "Working on batch #:  29\n",
      "loss_on_batch: 0.06899354606866837  time_on_batch: 92.70415616035461\n",
      "Working on batch #:  30\n",
      "loss_on_batch: 0.13089941442012787  time_on_batch: 64.51223278045654\n",
      "Working on batch #:  31\n",
      "loss_on_batch: 0.12227941304445267  time_on_batch: 48.98228096961975\n",
      "Working on batch #:  32\n",
      "loss_on_batch: 0.11934337019920349  time_on_batch: 62.20334553718567\n",
      "Working on batch #:  33\n",
      "loss_on_batch: 0.10321157425642014  time_on_batch: 61.667540311813354\n",
      "Working on batch #:  34\n",
      "loss_on_batch: 0.07854633778333664  time_on_batch: 61.22741508483887\n",
      "Working on batch #:  35\n",
      "loss_on_batch: 0.06875838339328766  time_on_batch: 66.50473213195801\n",
      "Working on batch #:  36\n",
      "loss_on_batch: 0.08034061640501022  time_on_batch: 61.51215386390686\n",
      "Working on batch #:  37\n",
      "loss_on_batch: 0.06526454538106918  time_on_batch: 286.3879361152649\n",
      "Working on batch #:  38\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-c067924e48a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mcandidates_qs_tuples_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstruct_qs_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_sequence_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mmain_qs_tuples_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstruct_qs_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestions_this_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_sequence_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0msimilarity_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidates_qs_tuples_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_qs_tuples_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-08\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-c067924e48a5>\u001b[0m in \u001b[0;36mconstruct_qs_matrix\u001b[1;34m(q_ids_sequential, dict_sequence_lengths, candidates)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mqs_padded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpadded_q_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_matrix_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mqs_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[0msum_h_qs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0mmean_pooled_h_qs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_h_qs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs_seq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mtanh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mTanh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtanh_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\autograd\\_functions\\pointwise.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, i, inplace)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' Model Specs '''\n",
    "# CNN parameters\n",
    "input_size = len(word2vec[list(word2vec.keys())[0]])\n",
    "hidden_size = 200\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 0\n",
    "dilation = 1\n",
    "groups = 1\n",
    "bias = True\n",
    "\n",
    "# CNN model\n",
    "cnn = torch.nn.Sequential()\n",
    "cnn.add_module('conv', torch.nn.Conv1d(in_channels = 200, out_channels = hidden_size, kernel_size = kernel_size, padding = padding, dilation = dilation, groups = groups, bias = bias))\n",
    "cnn.add_module('tanh', torch.nn.Tanh())\n",
    "\n",
    "# Loss function\n",
    "loss_function = torch.nn.MultiMarginLoss(margin=0.2)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=10**-2, weight_decay=0.001)\n",
    "\n",
    "\n",
    "''' Procedural parameters '''\n",
    "batch_size = 100\n",
    "num_differing_questions = 20\n",
    "\n",
    "num_epochs = 10\n",
    "num_batches = round(len(trainingQuestionIds)/batch_size)\n",
    "\n",
    "\n",
    "# Given ids of main qs in this batch\n",
    "#\n",
    "# Returns:\n",
    "# 1. ids in ordered list as: \n",
    "# [\n",
    "# q_1+, q_1-, q_1--,..., q_1++, q_1-, q_1--,...,\n",
    "# q_2+, q_2-, q_2--,..., q_2++, q_2-, q_2--,...,\n",
    "# ...\n",
    "# ]\n",
    "# All n main questions have their pos,neg,neg,neg,... interleaved\n",
    "#\n",
    "# 2. A dict mapping main question id --> its interleaved sequence length\n",
    "def order_ids(q_ids):\n",
    "    global training_data\n",
    "    global num_differing_questions\n",
    "\n",
    "    sequence_ids = []\n",
    "    dict_sequence_lengths = {}\n",
    "\n",
    "    for q_main in q_ids:\n",
    "        p_pluses = training_data[q_main][0]\n",
    "        p_minuses = list(np.random.choice(training_data[q_main][1], num_differing_questions, replace = False))\n",
    "        sequence_length = len(p_pluses) * num_differing_questions + len(p_pluses)\n",
    "        dict_sequence_lengths[q_main] = sequence_length\n",
    "        for p_plus in p_pluses:\n",
    "            sequence_ids += [p_plus] + p_minuses\n",
    "\n",
    "    return sequence_ids, dict_sequence_lengths\n",
    "\n",
    "\n",
    "'''Matrix constructors (use global vars, leave in order)'''\n",
    "# A tuple is (q+, q-, q--, q--- ...)\n",
    "# Let all main questions be set Q\n",
    "# Each q in Q has a number of tuples equal to number of positives |q+, q++, ...|\n",
    "# Each q in Q will have a 2D matrix of: num_tuples x num_candidates_in_tuple\n",
    "# Concatenate this matrix for all q in Q and you get a matrix of: |Q| x num_tuples x num_candidates_in_tuple\n",
    "\n",
    "# The above is for candidates\n",
    "# To do cosine_similarity, need same structure with q's\n",
    "# Basically each q will be a matrix of repeated q's: num_tuples x num_candidates_in_tuple, all elts are q (repeated)\n",
    "\n",
    "# This method constructs those matrices, use candidates=True for candidates matrix\n",
    "\n",
    "def construct_qs_matrix(q_ids_sequential, dict_sequence_lengths, candidates=False):\n",
    "    global cnn, h0, c0, word2vec, id2data, input_size, num_differing_questions\n",
    "\n",
    "    if not candidates:\n",
    "        q_ids_complete = []\n",
    "        for q in q_ids_sequential:\n",
    "            q_ids_complete += [q] * dict_sequence_lengths[q]\n",
    "\n",
    "    else: q_ids_complete = q_ids_sequential\n",
    "\n",
    "    qs_matrix_list = []\n",
    "    qs_seq_length = []\n",
    "\n",
    "    for q in q_ids_complete:\n",
    "        q_matrix_3d, q_num_words = get_question_matrix(q, word2vec, id2Data, input_size)\n",
    "        qs_matrix_list.append(q_matrix_3d)\n",
    "        qs_seq_length.append(q_num_words)\n",
    "\n",
    "    qs_padded = padded_q_matrix(qs_matrix_list)\n",
    "    qs_hidden = cnn(torch.transpose(qs_padded, 1, 2))\n",
    "    sum_h_qs = torch.sum(qs_hidden, dim=2)\n",
    "    mean_pooled_h_qs = torch.div(sum_h_qs, torch.autograd.Variable(torch.FloatTensor(qs_seq_length)[:, np.newaxis]))\n",
    "    qs_tuples = mean_pooled_h_qs.split(1+num_differing_questions)\n",
    "    final_matrix_tuples_by_constituent_qs_by_hidden_size = torch.stack(qs_tuples, dim=0, out=None)\n",
    "    return final_matrix_tuples_by_constituent_qs_by_hidden_size\n",
    "\n",
    "\n",
    "'''Begin training'''\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for batch in range(1, num_batches+1):\n",
    "        start = time.time()\n",
    "\n",
    "        print(\"Working on batch #: \", batch)\n",
    "        optimizer.zero_grad()\n",
    "        questions_this_batch = trainingQuestionIds[batch_size * (batch - 1):batch_size * batch]\n",
    "        sequence_ids, dict_sequence_lengths = order_ids(questions_this_batch)\n",
    "\n",
    "        candidates_qs_tuples_matrix = construct_qs_matrix(sequence_ids, dict_sequence_lengths, candidates=True)\n",
    "        main_qs_tuples_matrix = construct_qs_matrix(questions_this_batch, dict_sequence_lengths, candidates=False)\n",
    "\n",
    "        similarity_matrix = torch.nn.functional.cosine_similarity(candidates_qs_tuples_matrix, main_qs_tuples_matrix, dim=2, eps=1e-08)\n",
    "\n",
    "        target = Variable(torch.LongTensor([0] * int(len(sequence_ids)/(1+num_differing_questions))))\n",
    "        loss_batch = loss_function(similarity_matrix, target)\n",
    "\n",
    "        loss_batch.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"loss_on_batch:\", loss_batch.data[0], \" time_on_batch:\", time.time() - start)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
