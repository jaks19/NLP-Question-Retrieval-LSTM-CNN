{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on batch #  1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "there are no graph nodes that require computing gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-17b8746da26e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mmini_batch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mmini_batch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'through'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/3/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/3/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: there are no graph nodes that require computing gradients"
     ]
    }
   ],
   "source": [
    "from preprocess import *\n",
    "from get_q_matrices_functions import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules.distance import CosineSimilarity\n",
    "\n",
    "\n",
    "# Given a list of ids, compute the hidden layer for each of those questions\n",
    "# Ideal if need to work on a group, not just one question\n",
    "def mean_pooled_hidden_layers_for_ids(list_ids, input_size):\n",
    "    qs_matrix_list = []\n",
    "    qs_seq_length = []\n",
    "        \n",
    "    for q in list_ids:\n",
    "        q_matrix_3d = get_question_matrix(q, word2vec, id2Data)\n",
    "        qs_matrix_list.append(q_matrix_3d)\n",
    "        qs_seq_length.append(q_matrix_3d.shape[1])\n",
    "    \n",
    "    qs_padded = padded_q_matrix(qs_seq_length, qs_matrix_list, input_size)\n",
    "    qs_hidden = torch.nn.utils.rnn.pad_packed_sequence(lstm(qs_padded, (h0, c0))[0], batch_first=True)\n",
    "    sum_h_qs = torch.sum(qs_hidden[0], dim=1)\n",
    "    \n",
    "    lst_hidden = []\n",
    "    for i in range(len(sum_h_qs)): \n",
    "        h = sum_h_qs[i] / qs_seq_length[i]\n",
    "        lst_hidden.append(h)\n",
    "    return lst_hidden\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''' Data Prep '''\n",
    "\n",
    "# Organize training data\n",
    "training_data = training_id_to_similar_different()\n",
    "trainingQuestionIds = list(training_data.keys())\n",
    "\n",
    "word2vec = get_words_and_embeddings()\n",
    "id2Data = questionID_to_questionData()\n",
    "\n",
    "\n",
    "''' Model Specs '''\n",
    "\n",
    "# LSTM parameters\n",
    "input_size = len(word2vec[list(word2vec.keys())[0]])\n",
    "hidden_size = 100\n",
    "num_layers = 1\n",
    "bias = True\n",
    "batch_first = True\n",
    "dropout = 0.2\n",
    "bidirectional = False\n",
    "\n",
    "# LSTM model, classifier, loss\n",
    "lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "\n",
    "# Loss function\n",
    "loss_function = torch.nn.MultiMarginLoss(margin=0.2)\n",
    "\n",
    "# Optimizer init, linking to model\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=10**-4, weight_decay=0.001)\n",
    "\n",
    "# Cos-similatity\n",
    "cosSim = CosineSimilarity()\n",
    "\n",
    "''' Procedural parameters '''\n",
    "\n",
    "# Sampling numbers\n",
    "batch_size = 100\n",
    "num_differing_questions = 20\n",
    "\n",
    "# How much to train?\n",
    "num_epochs = 10\n",
    "num_batches = round(len(training_data.keys())/batch_size)\n",
    "\n",
    "# Save model after each epoch?\n",
    "pickle_each_epoch = True\n",
    "\n",
    "''' Begin Training '''\n",
    "h0 = Variable(torch.zeros(1, 1, hidden_size), requires_grad=True)\n",
    "c0 = Variable(torch.zeros(1, 1, hidden_size), requires_grad=True)\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(1, num_batches+2):\n",
    "        print(\"Working on batch # \", batch)\n",
    "        \n",
    "        # Init gradient and loss for this batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # All question ids for this batch\n",
    "        questions_this_batch = trainingQuestionIds[batch_size * (batch - 1):batch_size * batch]\n",
    "        \n",
    "        total_loss = torch.autograd.Variable(torch.zeros(1), requires_grad=True)\n",
    "        \n",
    "        for q in questions_this_batch:\n",
    "            \n",
    "            # Mean pooled hidden layer for q\n",
    "            q_matrix_3d = Variable(torch.from_numpy(get_question_matrix(q, word2vec, id2Data)))\n",
    "            q_hidden = lstm(q_matrix_3d, (h0, c0))[0]\n",
    "            avg_q_hidden = torch.sum(q_hidden, dim=1) / q_matrix_3d.size()[1]\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Space left because I had refactored this into the method implemented on top of this page,\n",
    "            # I moved code back in when I saw the no graph node error...\n",
    "            \n",
    "            \n",
    "            pos_qs = training_data[q][0]\n",
    "            num_pos = len(pos_qs)\n",
    "            \n",
    "            qs_matrix_list = []\n",
    "            qs_seq_length = []\n",
    "\n",
    "            for q_pos in pos_qs:\n",
    "                q_matrix_3d = get_question_matrix(q_pos, word2vec, id2Data)\n",
    "                qs_matrix_list.append(q_matrix_3d)\n",
    "                qs_seq_length.append(q_matrix_3d.shape[1])\n",
    "\n",
    "            qs_padded = padded_q_matrix(qs_seq_length, qs_matrix_list, input_size)\n",
    "            qs_hidden = torch.nn.utils.rnn.pad_packed_sequence(lstm(qs_padded, (h0, c0))[0], batch_first=True)\n",
    "            sum_h_qs = torch.sum(qs_hidden[0], dim=1)\n",
    "\n",
    "            lst_hidden = []\n",
    "            for i in range(len(sum_h_qs)): \n",
    "                h = sum_h_qs[i] / qs_seq_length[i]\n",
    "                lst_hidden.append(h)\n",
    "        \n",
    "            mean_pooled_hidden_layers_pos = list(lst_hidden)\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            # Again, Space left because I had refactored this into the method implemented on top of this page,\n",
    "            # I moved code back in when I saw the no graph node error...\n",
    "            \n",
    "            neg_qs = np.random.choice(training_data[q][1], num_differing_questions, replace = False)\n",
    "            num_neg = len(neg_qs)\n",
    "            \n",
    "            qs_matrix_list = []\n",
    "            qs_seq_length = []\n",
    "\n",
    "            for q_neg in neg_qs:\n",
    "                q_matrix_3d = get_question_matrix(q_neg, word2vec, id2Data)\n",
    "                qs_matrix_list.append(q_matrix_3d)\n",
    "                qs_seq_length.append(q_matrix_3d.shape[1])\n",
    "\n",
    "            qs_padded = padded_q_matrix(qs_seq_length, qs_matrix_list, input_size)\n",
    "            qs_hidden = torch.nn.utils.rnn.pad_packed_sequence(lstm(qs_padded, (h0, c0))[0], batch_first=True)\n",
    "            sum_h_qs = torch.sum(qs_hidden[0], dim=1)\n",
    "\n",
    "            lst_hidden = []\n",
    "            for i in range(len(sum_h_qs)): \n",
    "                h = sum_h_qs[i] / qs_seq_length[i]\n",
    "                lst_hidden.append(h)\n",
    "            \n",
    "            mean_pooled_hidden_layers_neg = list(mean_pooled_hidden_layers_for_ids(neg_qs, input_size))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # A tuple is (q+, q-, q--, q--- ...)\n",
    "            # Matrix of all tuples (num_tuples x num_qs_in_tuple x hidden_size)\n",
    "            \n",
    "            matrix_all_tuples = torch.FloatTensor(num_pos, 1+num_neg, hidden_size)\n",
    "            for i, h_pos in enumerate(mean_pooled_hidden_layers_pos):\n",
    "                this_tuple = torch.stack([h_pos]+mean_pooled_hidden_layers_neg)\n",
    "                matrix_all_tuples[i] = this_tuple.data\n",
    "            \n",
    "            matrix_q_only = torch.FloatTensor(num_pos, 1+num_neg, hidden_size)\n",
    "            for i in range(len(mean_pooled_hidden_layers_pos)):\n",
    "                this_tuple = torch.stack([avg_q_hidden]*(1+num_neg))\n",
    "                matrix_q_only[i] = this_tuple.data\n",
    "            \n",
    "            \n",
    "            similarity_matrix = torch.nn.functional.cosine_similarity(matrix_all_tuples, matrix_q_only, dim=2, eps=1e-08)\n",
    "            target = torch.LongTensor([0]*num_pos)\n",
    "            mini_batch_loss = loss_function(similarity_matrix, target)\n",
    "            mini_batch_loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"loss on this batch: \", batch_loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(neq_qs)\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:3]",
   "language": "python",
   "name": "conda-env-3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
