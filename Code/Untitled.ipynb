{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Global vars\n",
    "hidden_size = 100\n",
    "input_size = 200\n",
    "num_differing_questions = 20\n",
    "\n",
    "# Produces tensor [1 x num_words x input_size] for one particular question\n",
    "def get_question_matrix(questionID, word2vec, id2Data, input_size):\n",
    "    # Get the vector representation for each word in this question as list [v1,v2,v3,...]\n",
    "    q_word_vecs = []\n",
    "    for word in id2Data[questionID]:\n",
    "        try:\n",
    "            word_vec = np.array(word2vec[word]).astype(np.float32).reshape(len(word2vec[word]), -1)\n",
    "            q_word_vecs.append(word_vec)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    # num_words x dim_words\n",
    "    q_matrix = torch.Tensor(np.concatenate(q_word_vecs, axis=1).T)\n",
    "    num_words_found = q_matrix.size()[0]\n",
    "    \n",
    "    if num_words_found < 100:\n",
    "        padding_rows = torch.zeros(100-num_words_found, input_size)\n",
    "        q_matrix = torch.cat((q_matrix, padding_rows), 0)\n",
    "    \n",
    "    return [q_matrix.unsqueeze(0), num_words_found]\n",
    "\n",
    "\n",
    "# Given ids of main qs in this batch\n",
    "# Returns:\n",
    "# 1. ids in ordered list as: \n",
    "# [ q_1+, q_1-, q_1--,..., q_1++, q_1-, q_1--,...,\n",
    "# q_2+, q_2-, q_2--,..., q_2++, q_2-, q_2--,...,]\n",
    "# All n main questions have their pos,neg,neg,neg,... interleaved\n",
    "# 2. A dict mapping main question id --> its interleaved sequence length\n",
    "def organize_ids_training(q_ids, data):\n",
    "    sequence_ids = []\n",
    "    dict_sequence_lengths = {}\n",
    "    \n",
    "    for q_main in q_ids:\n",
    "        p_pluses = data[q_main][0]\n",
    "        p_minuses = list(np.random.choice(data[q_main][1], num_differing_questions, replace = False))\n",
    "        sequence_length = len(p_pluses) * num_differing_questions + len(p_pluses)\n",
    "        dict_sequence_lengths[q_main] = sequence_length\n",
    "        for p_plus in p_pluses:\n",
    "            sequence_ids += [p_plus] + p_minuses\n",
    "\n",
    "    return sequence_ids, dict_sequence_lengths\n",
    "\n",
    "\n",
    "# Given ids of main qs in this batch\n",
    "# Returns:\n",
    "# 1. ids of the 20 questions for each q_main\n",
    "# Note: Varying number of p_plus\n",
    "# 2. A dict mapping main question id --> its p_pluses ids\n",
    "def organize_test_ids(q_ids, data):\n",
    "    sequence_ids = []\n",
    "    dict_p_pluses = {}\n",
    "    \n",
    "    for i, q_main in enumerate(q_ids):\n",
    "        all_p = data[q_main][1]\n",
    "        p_pluses = data[q_main][0]\n",
    "        p_pluses_indices = []\n",
    "        for pos_id in p_pluses:\n",
    "            p_pluses_indices += [all_p.index(pos_id)] \n",
    "        sequence_ids += all_p\n",
    "        dict_p_pluses[i] = p_pluses_indices\n",
    "        \n",
    "    return sequence_ids, dict_p_pluses\n",
    "\n",
    "\n",
    "# A tuple is (q+, q-, q--, q--- ...)\n",
    "# Let all main questions be set Q\n",
    "# Each q in Q has a number of tuples equal to number of positives |q+, q++, ...|\n",
    "# Each q in Q will have a 2D matrix of: num_tuples x num_candidates_in_tuple\n",
    "# Concatenate this matrix for all q in Q and you get a matrix of: |Q| x num_tuples x num_candidates_in_tuple\n",
    "\n",
    "# The above is for candidates\n",
    "# To do cosine_similarity, need same structure with q's\n",
    "# Basically each q will be a matrix of repeated q's: num_tuples x num_candidates_in_tuple, all elts are q (repeated)\n",
    "\n",
    "# This method constructs those matrices, use candidates=True for candidates matrix\n",
    "def construct_qs_matrix_training(q_ids_sequential, lstm, h0, c0, word2vec, id2Data, dict_sequence_lengths, candidates=False):\n",
    "    if not candidates:\n",
    "        q_ids_complete = []\n",
    "        for q in q_ids_sequential:\n",
    "            q_ids_complete += [q] * dict_sequence_lengths[q]\n",
    "    \n",
    "    else: q_ids_complete = q_ids_sequential\n",
    "\n",
    "    qs_matrix_list = []\n",
    "    qs_seq_length = []\n",
    "    \n",
    "    for q in q_ids_complete:\n",
    "        q_matrix_3d, q_num_words = get_question_matrix(q, word2vec, id2Data, input_size)\n",
    "        qs_matrix_list.append(q_matrix_3d)\n",
    "        qs_seq_length.append(q_num_words)\n",
    "\n",
    "    qs_padded = Variable(torch.cat(qs_matrix_list, 0))\n",
    "    print('qs_padded', qs_padded)\n",
    "    qs_hidden = lstm(qs_padded, (h0, c0)) # [ [num_q, num_word_per_q, hidden_size] i.e. all hidden, [1, num_q, hidden_size]  i.e. final hidden]\n",
    "    print('qs_hidden', qs_hidden)\n",
    "    sum_h_qs = torch.sum(qs_hidden[0], dim=1)\n",
    "    print('sum', sum_h_qs)\n",
    "    mean_pooled_h_qs = torch.div(sum_h_qs, torch.autograd.Variable(torch.FloatTensor(qs_seq_length)[:, np.newaxis]))\n",
    "    print('mean', mean_pooled_h_qs)\n",
    "    qs_tuples = mean_pooled_h_qs.split(1+num_differing_questions)\n",
    "    print('tuples', qs_tuples)\n",
    "    final_matrix_tuples_by_constituent_qs_by_hidden_size = torch.stack(qs_tuples, dim=0, out=None)\n",
    "    print('final', final_matrix_tuples_by_constituent_qs_by_hidden_size)\n",
    "    sss\n",
    "    return final_matrix_tuples_by_constituent_qs_by_hidden_size\n",
    "\n",
    "\n",
    "# Case candidates: gives a matrix with a row for each q_main, with 20 p's\n",
    "# Case not candidates: gives a matrix with a row for each q_main, with 20 q_main's repeated\n",
    "def construct_qs_matrix_testing(q_ids_sequential, lstm, h0, c0, word2vec, id2Data, candidates=False):\n",
    "    num_ps_per_q = 20\n",
    "    \n",
    "    if not candidates:\n",
    "        q_ids_complete = []\n",
    "        for q in q_ids_sequential:\n",
    "            q_ids_complete += [q] * num_ps_per_q\n",
    "    \n",
    "    else: q_ids_complete = q_ids_sequential\n",
    "\n",
    "    qs_matrix_list = []\n",
    "    qs_seq_length = []\n",
    "    \n",
    "    for q in q_ids_complete:\n",
    "        q_matrix_3d, q_num_words = get_question_matrix(q, word2vec, id2Data, input_size)\n",
    "        qs_matrix_list.append(q_matrix_3d)\n",
    "        qs_seq_length.append(q_num_words)\n",
    "\n",
    "    qs_padded = Variable(torch.cat(qs_matrix_list, 0))\n",
    "    qs_hidden = lstm(qs_padded, (h0, c0)) # [ [num_q, num_word_per_q, hidden_size] i.e. all hidden, [1, num_q, hidden_size]  i.e. final hidden]\n",
    "    sum_h_qs = torch.sum(qs_hidden[0], dim=1)\n",
    "    mean_pooled_h_qs = torch.div(sum_h_qs, torch.autograd.Variable(torch.FloatTensor(qs_seq_length)[:, np.newaxis]))\n",
    "    qs_tuples = mean_pooled_h_qs.split(num_ps_per_q)\n",
    "    final_matrix_tuples_by_constituent_qs_by_hidden_size = torch.stack(qs_tuples, dim=0, out=None)\n",
    "    \n",
    "    return final_matrix_tuples_by_constituent_qs_by_hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on batch #:  1\n",
      "qs_padded Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.1442 -0.1867 -0.0002  ...   0.0169 -0.0526 -0.1357\n",
      "  0.1020 -0.1044 -0.0128  ...   0.0344 -0.0136 -0.0370\n",
      "  0.1425 -0.1104  0.0730  ...  -0.0451  0.0040  0.0159\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  0.0587 -0.1163  0.0160  ...   0.1748 -0.0052 -0.1124\n",
      " -0.0123 -0.1376 -0.0129  ...  -0.0811  0.0452  0.0052\n",
      "  0.0104 -0.0439  0.0399  ...   0.0976 -0.0064  0.0516\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "( 2 ,.,.) = \n",
      "  0.1019 -0.0890 -0.0525  ...  -0.0341  0.1238  0.1201\n",
      " -0.0385 -0.0178 -0.1324  ...  -0.0632  0.0141 -0.0471\n",
      "  0.0149  0.1627 -0.0706  ...   0.0349 -0.0390 -0.1047\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "... \n",
      "\n",
      "(39 ,.,.) = \n",
      "  0.1273 -0.0432  0.0310  ...   0.2174 -0.0273 -0.0238\n",
      "  0.1425 -0.1104  0.0730  ...  -0.0451  0.0040  0.0159\n",
      "  0.1600 -0.0816  0.0215  ...   0.0836 -0.0593  0.0188\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "(40 ,.,.) = \n",
      "  0.1162 -0.0406 -0.0299  ...   0.0280 -0.0782  0.0584\n",
      " -0.0229 -0.0970  0.0017  ...  -0.0034 -0.0115 -0.1356\n",
      " -0.1088  0.1172 -0.1394  ...   0.0013 -0.0157 -0.1159\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "(41 ,.,.) = \n",
      "  0.1273 -0.0432  0.0310  ...   0.2174 -0.0273 -0.0238\n",
      "  0.1425 -0.1104  0.0730  ...  -0.0451  0.0040  0.0159\n",
      "  0.0003 -0.0584  0.0777  ...  -0.0061 -0.0403 -0.0246\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 42x100x200]\n",
      "\n",
      "qs_hidden (Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  1.5043e-02 -9.0190e-03  3.9216e-03  ...  -1.6202e-02 -1.8416e-02  4.3359e-02\n",
      "  1.8553e-02 -4.5868e-03 -6.5334e-04  ...   2.5417e-03 -2.7599e-02  6.2289e-02\n",
      "  3.4347e-02 -2.2975e-02  1.7605e-02  ...   1.7416e-02 -2.9804e-02  6.3909e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  4.5309e-02  7.8234e-03  2.0413e-02  ...  -1.0236e-02 -2.7155e-03  7.0829e-02\n",
      "  4.5309e-02  7.8234e-03  2.0413e-02  ...  -1.0236e-02 -2.7155e-03  7.0829e-02\n",
      "  4.5309e-02  7.8234e-03  2.0413e-02  ...  -1.0236e-02 -2.7155e-03  7.0829e-02\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  2.3196e-02  1.3398e-02  3.3607e-02  ...   2.3771e-02 -1.8444e-02  5.3568e-02\n",
      "  3.5401e-02 -1.6021e-03  2.0453e-02  ...  -1.4629e-02 -2.8740e-02  8.2137e-02\n",
      "  4.1146e-02  8.7408e-03  4.0702e-02  ...  -8.3749e-03 -1.2371e-02  9.2677e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  4.5309e-02  7.8233e-03  2.0413e-02  ...  -1.0236e-02 -2.7157e-03  7.0829e-02\n",
      "  4.5309e-02  7.8233e-03  2.0413e-02  ...  -1.0236e-02 -2.7156e-03  7.0829e-02\n",
      "  4.5309e-02  7.8233e-03  2.0413e-02  ...  -1.0236e-02 -2.7155e-03  7.0829e-02\n",
      "\n",
      "( 2 ,.,.) = \n",
      "  1.0844e-02 -1.5519e-02  8.5488e-03  ...  -5.0286e-02 -8.9975e-03  4.5377e-02\n",
      "  3.0042e-02 -2.3530e-03  1.2395e-02  ...  -3.6048e-02 -1.1966e-02  8.1578e-02\n",
      "  4.2501e-02  7.5178e-03  3.3404e-02  ...  -3.9003e-02 -9.2022e-03  7.8563e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  4.4685e-02  6.6169e-03  2.0484e-02  ...  -7.8547e-03 -4.3229e-03  7.2899e-02\n",
      "  4.4893e-02  7.1372e-03  2.0467e-02  ...  -8.7347e-03 -3.7554e-03  7.2105e-02\n",
      "  4.5041e-02  7.4298e-03  2.0467e-02  ...  -9.2876e-03 -3.3813e-03  7.1618e-02\n",
      "... \n",
      "\n",
      "(39 ,.,.) = \n",
      "  6.0065e-03  3.5747e-03  7.3121e-03  ...   2.0601e-02 -2.4787e-03  5.9430e-02\n",
      "  2.3861e-02 -1.9678e-02  2.0552e-02  ...   2.5761e-02 -1.7395e-02  6.4749e-02\n",
      "  3.6169e-02 -3.5543e-03  7.3145e-03  ...   5.0604e-03  2.2152e-03  1.0421e-01\n",
      "                 ...                   ⋱                   ...                \n",
      "  4.5106e-02  7.5664e-03  1.9195e-02  ...  -8.7662e-03 -2.7486e-03  7.1385e-02\n",
      "  4.5221e-02  7.7513e-03  1.9738e-02  ...  -9.3844e-03 -2.7675e-03  7.1209e-02\n",
      "  4.5278e-02  7.8290e-03  2.0057e-02  ...  -9.7515e-03 -2.7694e-03  7.1086e-02\n",
      "\n",
      "(40 ,.,.) = \n",
      "  2.7209e-02  3.6197e-03  1.8165e-02  ...   3.0848e-03 -3.5996e-03  3.9482e-02\n",
      "  2.6343e-02  6.0063e-03  1.6932e-02  ...   5.0102e-03 -3.4031e-02  8.6691e-02\n",
      "  3.7620e-02  1.5001e-02  3.0570e-02  ...  -9.7340e-03 -3.0070e-02  8.8578e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  4.5309e-02  7.8234e-03  2.0413e-02  ...  -1.0235e-02 -2.7157e-03  7.0829e-02\n",
      "  4.5309e-02  7.8234e-03  2.0413e-02  ...  -1.0236e-02 -2.7156e-03  7.0829e-02\n",
      "  4.5309e-02  7.8234e-03  2.0413e-02  ...  -1.0236e-02 -2.7156e-03  7.0829e-02\n",
      "\n",
      "(41 ,.,.) = \n",
      "  6.0065e-03  3.5747e-03  7.3121e-03  ...   2.0601e-02 -2.4787e-03  5.9430e-02\n",
      "  2.3861e-02 -1.9678e-02  2.0552e-02  ...   2.5761e-02 -1.7395e-02  6.4749e-02\n",
      "  2.5366e-02 -1.5093e-02  4.6645e-03  ...   4.6400e-03 -3.2350e-02  8.3591e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  4.5482e-02  7.8694e-03  2.0150e-02  ...  -1.0030e-02 -2.7404e-03  7.0881e-02\n",
      "  4.5385e-02  7.8654e-03  2.0240e-02  ...  -1.0163e-02 -2.7396e-03  7.0872e-02\n",
      "  4.5339e-02  7.8540e-03  2.0304e-02  ...  -1.0225e-02 -2.7350e-03  7.0865e-02\n",
      "[torch.FloatTensor of size 42x100x100]\n",
      ", (Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.0453  0.0078  0.0204  ...  -0.0102 -0.0027  0.0708\n",
      "  0.0453  0.0078  0.0204  ...  -0.0102 -0.0027  0.0708\n",
      "  0.0450  0.0074  0.0205  ...  -0.0093 -0.0034  0.0716\n",
      "           ...             ⋱             ...          \n",
      "  0.0453  0.0078  0.0201  ...  -0.0098 -0.0028  0.0711\n",
      "  0.0453  0.0078  0.0204  ...  -0.0102 -0.0027  0.0708\n",
      "  0.0453  0.0079  0.0203  ...  -0.0102 -0.0027  0.0709\n",
      "[torch.FloatTensor of size 1x42x100]\n",
      ", Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.0959  0.0160  0.0411  ...  -0.0191 -0.0057  0.1398\n",
      "  0.0959  0.0160  0.0411  ...  -0.0191 -0.0057  0.1398\n",
      "  0.0953  0.0152  0.0412  ...  -0.0173 -0.0070  0.1414\n",
      "           ...             ⋱             ...          \n",
      "  0.0958  0.0160  0.0403  ...  -0.0182 -0.0058  0.1403\n",
      "  0.0959  0.0160  0.0411  ...  -0.0191 -0.0057  0.1398\n",
      "  0.0959  0.0161  0.0408  ...  -0.0191 -0.0057  0.1399\n",
      "[torch.FloatTensor of size 1x42x100]\n",
      "))\n",
      "sum Variable containing:\n",
      " 4.7238e+00  4.4274e-01  1.6485e+00  ...   3.9837e-01 -5.9497e-01  7.4907e+00\n",
      " 4.9768e+00 -1.4230e-02  1.2269e+00  ...   1.9932e-02 -6.0512e-01  7.7679e+00\n",
      " 3.6846e+00  5.8183e-01  1.5778e+00  ...   1.0685e+00 -5.0932e-01  7.7468e+00\n",
      "                ...                   ⋱                   ...                \n",
      " 4.7051e+00 -1.1253e-01  1.0634e+00  ...   1.4803e+00 -7.6256e-02  7.8789e+00\n",
      " 4.5702e+00  4.4905e-01  2.1217e+00  ...  -2.0885e-01 -3.6653e-01  7.4083e+00\n",
      " 4.1030e+00 -2.3210e-01  9.6605e-01  ...   9.0834e-01 -5.9503e-01  7.7331e+00\n",
      "[torch.FloatTensor of size 42x100]\n",
      "\n",
      "mean Variable containing:\n",
      " 6.3835e-02  5.9829e-03  2.2277e-02  ...   5.3833e-03 -8.0401e-03  1.0123e-01\n",
      " 6.7255e-02 -1.9230e-04  1.6580e-02  ...   2.6935e-04 -8.1774e-03  1.0497e-01\n",
      " 3.9619e-02  6.2562e-03  1.6966e-02  ...   1.1489e-02 -5.4766e-03  8.3299e-02\n",
      "                ...                   ⋱                   ...                \n",
      " 5.1142e-02 -1.2231e-03  1.1559e-02  ...   1.6090e-02 -8.2887e-04  8.5640e-02\n",
      " 6.3475e-02  6.2368e-03  2.9468e-02  ...  -2.9008e-03 -5.0907e-03  1.0289e-01\n",
      " 4.5589e-02 -2.5789e-03  1.0734e-02  ...   1.0093e-02 -6.6115e-03  8.5923e-02\n",
      "[torch.FloatTensor of size 42x100]\n",
      "\n",
      "tuples (Variable containing:\n",
      " 6.3835e-02  5.9829e-03  2.2277e-02  ...   5.3833e-03 -8.0401e-03  1.0123e-01\n",
      " 6.7255e-02 -1.9230e-04  1.6580e-02  ...   2.6935e-04 -8.1774e-03  1.0497e-01\n",
      " 3.9619e-02  6.2562e-03  1.6966e-02  ...   1.1489e-02 -5.4766e-03  8.3299e-02\n",
      "                ...                   ⋱                   ...                \n",
      " 4.4764e-02 -6.5950e-04  8.5717e-03  ...   1.8731e-02 -6.5698e-03  9.5648e-02\n",
      " 5.4129e-02  3.5019e-03  1.6695e-02  ...   1.4487e-02 -6.1179e-03  7.9611e-02\n",
      " 5.2914e-02  8.2573e-03  1.1771e-02  ...   1.5974e-03 -3.5808e-04  8.6147e-02\n",
      "[torch.FloatTensor of size 21x100]\n",
      ", Variable containing:\n",
      " 0.1012  0.0211  0.0385  ...  -0.0070 -0.0059  0.1854\n",
      " 0.0514  0.0056  0.0123  ...   0.0062 -0.0092  0.0891\n",
      " 0.0667  0.0081  0.0158  ...   0.0121 -0.0099  0.1202\n",
      "          ...             ⋱             ...          \n",
      " 0.0511 -0.0012  0.0116  ...   0.0161 -0.0008  0.0856\n",
      " 0.0635  0.0062  0.0295  ...  -0.0029 -0.0051  0.1029\n",
      " 0.0456 -0.0026  0.0107  ...   0.0101 -0.0066  0.0859\n",
      "[torch.FloatTensor of size 21x100]\n",
      ")\n",
      "final Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  6.3835e-02  5.9829e-03  2.2277e-02  ...   5.3833e-03 -8.0401e-03  1.0123e-01\n",
      "  6.7255e-02 -1.9230e-04  1.6580e-02  ...   2.6935e-04 -8.1774e-03  1.0497e-01\n",
      "  3.9619e-02  6.2562e-03  1.6966e-02  ...   1.1489e-02 -5.4766e-03  8.3299e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  4.4764e-02 -6.5950e-04  8.5717e-03  ...   1.8731e-02 -6.5698e-03  9.5648e-02\n",
      "  5.4129e-02  3.5019e-03  1.6695e-02  ...   1.4487e-02 -6.1179e-03  7.9611e-02\n",
      "  5.2914e-02  8.2573e-03  1.1771e-02  ...   1.5974e-03 -3.5808e-04  8.6147e-02\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  1.0123e-01  2.1126e-02  3.8472e-02  ...  -6.9711e-03 -5.9278e-03  1.8543e-01\n",
      "  5.1365e-02  5.6490e-03  1.2343e-02  ...   6.1839e-03 -9.1534e-03  8.9106e-02\n",
      "  6.6659e-02  8.1331e-03  1.5771e-02  ...   1.2135e-02 -9.9310e-03  1.2018e-01\n",
      "                 ...                   ⋱                   ...                \n",
      "  5.1142e-02 -1.2231e-03  1.1559e-02  ...   1.6090e-02 -8.2887e-04  8.5640e-02\n",
      "  6.3475e-02  6.2368e-03  2.9468e-02  ...  -2.9008e-03 -5.0907e-03  1.0289e-01\n",
      "  4.5589e-02 -2.5789e-03  1.0734e-02  ...   1.0093e-02 -6.6115e-03  8.5923e-02\n",
      "[torch.FloatTensor of size 2x21x100]\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e8321b33df80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mquestions_this_training_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingQuestionIds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Working on batch #: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestions_this_training_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2Data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# Evaluate on dev and test sets for MRR score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e8321b33df80>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(lstm, optimizer, batch_ids, batch_data, word2vec, id2Data, dev_set)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0msequence_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_sequence_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morganize_ids_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mcandidates_qs_tuples_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_qs_matrix_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2Data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_sequence_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mmain_qs_tuples_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_qs_matrix_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2Data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_sequence_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0msimilarity_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates_qs_tuples_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_qs_tuples_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-0f59962908e8>\u001b[0m in \u001b[0;36mconstruct_qs_matrix_training\u001b[0;34m(q_ids_sequential, lstm, h0, c0, word2vec, id2Data, dict_sequence_lengths, candidates)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mfinal_matrix_tuples_by_constituent_qs_by_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs_tuples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'final'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_matrix_tuples_by_constituent_qs_by_hidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0msss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_matrix_tuples_by_constituent_qs_by_hidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sss' is not defined"
     ]
    }
   ],
   "source": [
    "from preprocess import *\n",
    "from scoring_metrics import *\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "\n",
    "saved_model_name = \"bestbest\"\n",
    "\n",
    "\n",
    "'''Hyperparams dashboard'''\n",
    "dropout = 0.2\n",
    "margin = 0.4\n",
    "lr = 10**-3\n",
    "\n",
    "\n",
    "''' Data Prep '''\n",
    "word2vec = get_words_and_embeddings()\n",
    "id2Data = questionID_to_questionData_truncate(100)\n",
    "\n",
    "training_data = training_id_to_similar_different()\n",
    "trainingQuestionIds = list(training_data.keys())[:100]\n",
    "\n",
    "dev_data = devTest_id_to_similar_different(dev=True)\n",
    "dev_question_ids = list(dev_data.keys())[:20]\n",
    "\n",
    "test_data = devTest_id_to_similar_different(dev=False)\n",
    "test_question_ids = list(test_data.keys())\n",
    "\n",
    "\n",
    "''' Model Specs '''\n",
    "input_size = len(word2vec[list(word2vec.keys())[0]])\n",
    "hidden_size = 100\n",
    "num_layers = 1\n",
    "bias = True\n",
    "batch_first = True\n",
    "bidirectional = False\n",
    "\n",
    "lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional)\n",
    "loss_function = torch.nn.MultiMarginLoss(margin=margin)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=lr)\n",
    "\n",
    "first_dim = num_layers * 2 if bidirectional else num_layers\n",
    "h0 = Variable(torch.zeros(1, 1, hidden_size), requires_grad=True)\n",
    "c0 = Variable(torch.zeros(1, 1, hidden_size), requires_grad=True)\n",
    "\n",
    "''' Procedural parameters '''\n",
    "batch_size = 2\n",
    "num_differing_questions = 20\n",
    "num_epochs = 10\n",
    "num_batches = round(len(trainingQuestionIds)/batch_size)\n",
    "\n",
    "def train_model(lstm, optimizer, batch_ids, batch_data, word2vec, id2Data, dev_set=False):\n",
    "    lstm.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    sequence_ids, dict_sequence_lengths = organize_ids_training(batch_ids, batch_data)\n",
    "\n",
    "    candidates_qs_tuples_matrix = construct_qs_matrix_training(sequence_ids, lstm, h0, c0, word2vec, id2Data, dict_sequence_lengths, candidates=True)\n",
    "    main_qs_tuples_matrix = construct_qs_matrix_training(batch_ids, lstm, h0, c0, word2vec, id2Data, dict_sequence_lengths, candidates=False)\n",
    "    similarity_matrix = torch.nn.functional.cosine_similarity(candidates_qs_tuples_matrix, main_qs_tuples_matrix, dim=2, eps=1e-08)\n",
    "\n",
    "    target = Variable(torch.LongTensor([0] * int(len(sequence_ids)/(1+num_differing_questions))))\n",
    "    loss_batch = loss_function(similarity_matrix, target)\n",
    "\n",
    "    loss_batch.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if dev_set: print(\"Trained on dev set with loss:\", loss_batch.data[0], \" time_on_batch:\", time.time() - start)\n",
    "    else: print(\"loss_on_batch:\", loss_batch.data[0], \" time_on_batch:\", time.time() - start)\n",
    "    return\n",
    "\n",
    "def eval_model(lstm, ids, data, word2vec, id2Data):\n",
    "    lstm.eval()\n",
    "    sequence_ids, p_pluses_indices_dict = organize_test_ids(ids, data)\n",
    "\n",
    "    candidates_qs_tuples_matrix = construct_qs_matrix_testing(sequence_ids, lstm, h0, c0, word2vec, id2Data, candidates=True)\n",
    "    main_qs_tuples_matrix = construct_qs_matrix_testing(ids, lstm, h0, c0, word2vec, id2Data, candidates=False)\n",
    "\n",
    "    similarity_matrix = torch.nn.functional.cosine_similarity(candidates_qs_tuples_matrix, main_qs_tuples_matrix, dim=2, eps=1e-08)\n",
    "    MRR_score = get_MRR_score(similarity_matrix, p_pluses_indices_dict)\n",
    "    return MRR_score\n",
    "\n",
    "\n",
    "'''Begin training'''\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Train on whole training data set\n",
    "    for batch in range(1, num_batches+1):\n",
    "        start = time.time()\n",
    "        questions_this_training_batch = trainingQuestionIds[batch_size * (batch - 1):batch_size * batch]\n",
    "        print(\"Working on batch #: \", batch)\n",
    "        train_model(lstm, optimizer, questions_this_training_batch, training_data, word2vec, id2Data, dev_set=False)\n",
    "        \n",
    "    # Evaluate on dev and test sets for MRR score\n",
    "    dev_MRR_score = eval_model(lstm, dev_question_ids, dev_data, word2vec, id2Data)\n",
    "    test_MRR_score = eval_model(lstm, test_question_ids, test_data, word2vec, id2Data)\n",
    "    print(\"MRR score on dev set:\", dev_MRR_score)\n",
    "    print(\"MRR score on test set:\", test_MRR_score)\n",
    "\n",
    "    # Log results to local logs.txt file\n",
    "    with open('logs.txt', 'a') as log_file:\n",
    "        log_file.write('epoch: ' + str(epoch) + '\\n')\n",
    "        log_file.write('lr: ' + str(lr) +  ' marg: ' + str(margin) + ' drop: ' + str(dropout) + '\\n' )        \n",
    "        log_file.write('dev_MRR: ' +  str(dev_MRR_score) + '\\n')\n",
    "        log_file.write('test_MRR: ' +  str(test_MRR_score) + '\\n')\n",
    "\n",
    "    # Save model for this epoch\n",
    "    # torch.save(lstm, '../Pickle/' + saved_model_name + '_epoch' + str(epoch) + '.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
