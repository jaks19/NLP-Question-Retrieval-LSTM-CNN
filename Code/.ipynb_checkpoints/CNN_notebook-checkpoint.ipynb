{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "from get_q_matrices_functions import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "''' Data Prep '''\n",
    "\n",
    "# Organize training data\n",
    "training_data = training_id_to_similar_different()\n",
    "trainingQuestionIds = list(training_data.keys())\n",
    "\n",
    "# Organize tools to fetch question test\n",
    "word2vec = get_words_and_embeddings()\n",
    "id2Data = questionID_to_questionData()\n",
    "\n",
    "\n",
    "''' Model Specs '''\n",
    "\n",
    "# CNN parameters\n",
    "input_size = len(word2vec[list(word2vec.keys())[0]])\n",
    "hidden_size = 100\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 0\n",
    "dilation = 1\n",
    "bias = True\n",
    "\n",
    "# CNN model\n",
    "cnn = torch.nn.Sequential()\n",
    "cnn.add_module('conv', torch.nn.Conv1d(in_channels = 200, out_channels = hidden_size, kernel_size = kernel_size, padding = padding, dilation = dilation, groups = groups, bias = bias))\n",
    "cnn.add_module('tanh', torch.nn.Tanh())\n",
    "\n",
    "# Loss function\n",
    "loss_function = torch.nn.MarginRankingLoss(margin=0.2, size_average=False)\n",
    "\n",
    "# Optimizer init, linking to model\n",
    "weight_decay = 0.001\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.01, weight_decay=0.001)\n",
    "\n",
    "\n",
    "''' Procedural parameters '''\n",
    "\n",
    "# Sampling numbers\n",
    "batch_size = 100\n",
    "num_differing_questions = 20\n",
    "\n",
    "# How much to train?\n",
    "num_epochs = 10\n",
    "num_batches = round(len(training_data.keys())/batch_size)\n",
    "\n",
    "# Save model after each epoch?\n",
    "pickle_each_epoch = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on batch #  1\n",
      "loss on this batch:  35.09769821166992\n",
      "Working on batch #  2\n",
      "loss on this batch:  63.848548889160156\n",
      "Working on batch #  3\n",
      "loss on this batch:  43.54364013671875\n",
      "Working on batch #  4\n",
      "loss on this batch:  38.31463623046875\n",
      "Working on batch #  5\n",
      "loss on this batch:  58.052581787109375\n",
      "Working on batch #  6\n",
      "loss on this batch:  31.1290225982666\n",
      "Working on batch #  7\n",
      "loss on this batch:  30.496381759643555\n",
      "Working on batch #  8\n",
      "loss on this batch:  26.4494686126709\n",
      "Working on batch #  9\n",
      "loss on this batch:  23.463956832885742\n"
     ]
    }
   ],
   "source": [
    "''' Begin Training '''\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(1, num_batches+2):\n",
    "        print(\"Working on batch # \", batch)\n",
    "        # All question ids for this batch\n",
    "        questions_this_batch = trainingQuestionIds[batch_size * (batch - 1):batch_size * batch]\n",
    "\n",
    "        # Init gradient and loss for this batch\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = Variable(torch.zeros(1).float())\n",
    "\n",
    "        for q in questions_this_batch:\n",
    "            # Grab positives and negatives, turn them into their respective matrices\n",
    "            pos_qs = training_data[q][0]\n",
    "            neg_qs = np.random.choice(training_data[q][1], num_differing_questions, replace = False)\n",
    "            \n",
    "            \n",
    "            # Build q matrix\n",
    "            # Get hidden layer for q normalized by num_words\n",
    "            q_matrix_3d = torch.transpose(Variable(torch.from_numpy(get_question_matrix(q, word2vec, id2Data))), 1, 2)\n",
    "            q_cnn_out = cnn(q_matrix_3d)\n",
    "            normalized_q_hidden = (torch.sum(q_cnn_out, dim = 2) / q_matrix_3d.size()[2]).squeeze(0)\n",
    "            \n",
    "            \n",
    "            # Build each p_plus matrix\n",
    "            # Get hidden layer for each p_plus normalized by num_words\n",
    "            # Get cosine similarity between each p_plus and q\n",
    "            # Each pair is one basis of comparison against all the negatives\n",
    "            num_hidden_layers_in_all = len(pos_qs)\n",
    "            score_pos_qs = Variable(torch.zeros(num_hidden_layers_in_all).float())\n",
    "            for i in range(len(pos_qs)):\n",
    "                pos_q_matrix_3d = torch.transpose(Variable(torch.from_numpy(get_question_matrix(pos_qs[i], word2vec, id2Data))), 1, 2)\n",
    "                p_plus_cnn_out = cnn.forward(pos_q_matrix_3d).squeeze(0)\n",
    "                normalized_p_plus_hidden = (torch.sum(p_plus_cnn_out, dim = 1) / pos_q_matrix_3d.size()[2])\n",
    "                score_pos_qs[i] = F.cosine_similarity(normalized_p_plus_hidden, normalized_q_hidden, dim = 0)\n",
    "            \n",
    "            \n",
    "            # Build each p_minus matrix\n",
    "            # Get hidden layer for each p_minus normalized by num_words\n",
    "            # Get cosine similarity between each p_minus and q\n",
    "            # Retrieve the max of all neg cos sims\n",
    "            num_hidden_layers_in_all = len(neg_qs)\n",
    "            maxi_score = Variable(torch.ones(1)) * -100000\n",
    "            for i in range(len(neg_qs)):\n",
    "                neg_q_matrix_3d = torch.transpose(Variable(torch.from_numpy(get_question_matrix(neg_qs[i], word2vec, id2Data))), 1, 2)\n",
    "                p_minus_cnn_out = cnn.forward(neg_q_matrix_3d).squeeze(0)\n",
    "                normalized_p_minus_hidden = (torch.sum(p_minus_cnn_out, dim = 1) / neg_q_matrix_3d.size()[2])\n",
    "                score = F.cosine_similarity(normalized_p_minus_hidden, normalized_q_hidden, dim = 0)\n",
    "                if score.data[0] > maxi_score.data[0]: \n",
    "                    maxi_score = score\n",
    "            max_neg_cos_sim_variable = maxi_score\n",
    "            \n",
    "            \n",
    "            # For each (q,p_plus) pair, get loss by comparing cos_sim (q,p_plus) v/s max[(q,p_minus)] for all p_minus\n",
    "            for score_q_p_plus in score_pos_qs:\n",
    "                batch_loss += loss_function.forward(score_q_p_plus,\n",
    "                                                    max_neg_cos_sim_variable,\n",
    "                                                    Variable(torch.ones(1)))\n",
    "\n",
    "        # Optimize model based on losses\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"loss on this batch: \", batch_loss.data[0])\n",
    "\n",
    "\n",
    "    ''' Pickle Model after each epoch'''\n",
    "\n",
    "    # Note: Model cannot be re-trained, but can be loaded for evaluation (See below)\n",
    "    if pickle_each_epoch:\n",
    "        torch.save(cnn, '../Pickle/CNN_epoch' + str(epoch) + '.pt')\n",
    "\n",
    "\n",
    "# Notes:\n",
    "# Run these line in evaluation scripts to load model:\n",
    "# lstm_loaded = torch.load('filename.pt')\n",
    "# lstm_loaded.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
