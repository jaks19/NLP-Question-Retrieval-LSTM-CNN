LSTM Notes

Here is what I am doing in train_LSTM.py:

- Converting question text to matrix form and putting this into LSTM
- Averaging over hidden layers outputted

- Converting associated postive question text to matrix form (the function that does this is in get_q_matrices_functions.py) and putting this into LSTM
- Averaging over hidden layers outputted

- Computing the cosine similarity between question and positive question

- Randomly sampling 20 associated negative questions, getting texts, and converting to matrix form
- Inputting all of these questions into LSTM at once (this involves stuff with padding and another function in get_q_matrices_functions.py)
- Averaging over hidden layers for each question

- Computing cosine similarities between question and all negative questions

- Computing the loss (input all cosine similarities for negative questions and cosine similarity for postive question)

- I current have it such that I do this process for 100 questions (batch_size = 100), add up the losses, and then call .backward()
- I also have code commented out where I call .backward() after each question (batch_size = 1)


The .backward() call is really slow, and the loss increased when I let it run for a bit, so clearly something must be wrong here.
I think I have done everything as I outlined, so I'm not sure what is wrong.

Also, I've been told it's a bad practice to upload Jupyter notebooks, but I've uploaded mine anyway in case you wanna take a look, lol.

If you have a more efficient way of implementing this, feel free to change any of the code!



------
Second round of coding:
- Training and loss followed: 394, 332, 201, 156, 259, 200, 205, 142, 175, 108,
- Need to implement: instead of taking cosine sim of all positives and averaging the result, take q and one of its p+,
compare to all p-. Then move to the next p+ for same q and compare to all p-. Until all p+ is done then move to NEXT q.

- 